{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNsQoMeeVTr3Ba/UUio8SKu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tubagokhan/RedScore/blob/main/CosineLexixalNLIV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgS9Gyg8Ow3H",
        "outputId": "27c097ef-00c4-455f-99a1-5b266af4f018"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5UNt1vYL9Ej",
        "outputId": "5a73e50d-32ac-4ab2-92e8-d2c0523db388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "Loading NLI model: roberta-large-mnli\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-large-mnli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/pipelines/text_classification.py:106: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading input file: /content/SampleRedundancyCases.xlsx\n",
            "Starting processing with both NLI methods...\n",
            "\n",
            "Processing row 1...\n",
            "\n",
            "Processing row 2...\n",
            "\n",
            "Processing row 3...\n",
            "\n",
            "Processing row 4...\n",
            "\n",
            "Processing row 5...\n",
            "\n",
            "Processing row 6...\n",
            "\n",
            "Processing row 7...\n",
            "\n",
            "Processing row 8...\n",
            "\n",
            "Processing row 9...\n",
            "\n",
            "Processing row 10...\n",
            "\n",
            "Processing row 11...\n",
            "\n",
            "Processing row 12...\n",
            "\n",
            "Processing row 13...\n",
            "\n",
            "Processing row 14...\n",
            "\n",
            "Processing row 15...\n",
            "\n",
            "Processing row 16...\n",
            "\n",
            "Processing row 17...\n",
            "\n",
            "Processing row 18...\n",
            "\n",
            "Processing row 19...\n",
            "\n",
            "Processing row 20...\n",
            "\n",
            "Processing row 21...\n",
            "\n",
            "Processing row 22...\n",
            "\n",
            "Processing row 23...\n",
            "\n",
            "Processing row 24...\n",
            "\n",
            "Processing row 25...\n",
            "\n",
            "Processing row 26...\n",
            "\n",
            "Processing row 27...\n",
            "\n",
            "Processing row 28...\n",
            "\n",
            "Processing row 29...\n",
            "\n",
            "Processing row 30...\n",
            "\n",
            "Processing row 31...\n",
            "\n",
            "Processing row 32...\n",
            "\n",
            "Processing row 33...\n",
            "\n",
            "Processing row 34...\n",
            "\n",
            "Processing row 35...\n",
            "\n",
            "Processing row 36...\n",
            "\n",
            "Processing row 37...\n",
            "\n",
            "Processing row 38...\n",
            "\n",
            "Processing row 39...\n",
            "\n",
            "Processing row 40...\n",
            "\n",
            "Processing row 41...\n",
            "\n",
            "Processing row 42...\n",
            "\n",
            "Processing row 43...\n",
            "\n",
            "Processing row 44...\n",
            "\n",
            "Processing row 45...\n",
            "\n",
            "Processing row 46...\n",
            "\n",
            "Processing row 47...\n",
            "\n",
            "Processing row 48...\n",
            "\n",
            "Processing row 49...\n",
            "\n",
            "Processing row 50...\n",
            "\n",
            "Processing row 51...\n",
            "\n",
            "Processing row 52...\n",
            "\n",
            "Processing row 53...\n",
            "\n",
            "Processing row 54...\n",
            "\n",
            "Processing row 55...\n",
            "\n",
            "Processing row 56...\n",
            "\n",
            "Processing row 57...\n",
            "\n",
            "Processing row 58...\n",
            "\n",
            "Processing row 59...\n",
            "\n",
            "Processing row 60...\n",
            "\n",
            "Processing row 61...\n",
            "\n",
            "Processing row 62...\n",
            "\n",
            "Processing row 63...\n",
            "\n",
            "Processing row 64...\n",
            "\n",
            "Processing row 65...\n",
            "\n",
            "Processing row 66...\n",
            "\n",
            "Processing row 67...\n",
            "\n",
            "Processing row 68...\n",
            "\n",
            "Processing row 69...\n",
            "\n",
            "Processing row 70...\n",
            "\n",
            "Processing row 71...\n",
            "\n",
            "Processing row 72...\n",
            "\n",
            "Processing row 73...\n",
            "\n",
            "Processing row 74...\n",
            "\n",
            "Processing row 75...\n",
            "\n",
            "Processing row 76...\n",
            "\n",
            "Processing row 77...\n",
            "\n",
            "Processing row 78...\n",
            "\n",
            "Processing row 79...\n",
            "\n",
            "Processing row 80...\n",
            "\n",
            "Processing row 81...\n",
            "\n",
            "Processing row 82...\n",
            "\n",
            "Processing row 83...\n",
            "\n",
            "Processing row 84...\n",
            "\n",
            "Processing row 85...\n",
            "\n",
            "Processing row 86...\n",
            "\n",
            "Processing row 87...\n",
            "\n",
            "Processing row 88...\n",
            "\n",
            "Processing row 89...\n",
            "\n",
            "Processing row 90...\n",
            "\n",
            "Processing row 91...\n",
            "\n",
            "Processing row 92...\n",
            "\n",
            "Processing row 93...\n",
            "\n",
            "Processing row 94...\n",
            "\n",
            "Processing row 95...\n",
            "\n",
            "Processing row 96...\n",
            "\n",
            "Processing row 97...\n",
            "\n",
            "Processing row 98...\n",
            "\n",
            "Processing row 99...\n",
            "\n",
            "Processing row 100...\n",
            "\n",
            "Processing row 101...\n",
            "\n",
            "Processing row 102...\n",
            "\n",
            "Processing row 103...\n",
            "\n",
            "Processing row 104...\n",
            "\n",
            "Processing row 105...\n",
            "\n",
            "Processing row 106...\n",
            "\n",
            "Processing row 107...\n",
            "\n",
            "Processing row 108...\n",
            "\n",
            "Processing row 109...\n",
            "\n",
            "Processing row 110...\n",
            "\n",
            "Processing row 111...\n",
            "\n",
            "Processing row 112...\n",
            "\n",
            "Processing row 113...\n",
            "\n",
            "Processing row 114...\n",
            "\n",
            "Processing row 115...\n",
            "\n",
            "Processing row 116...\n",
            "\n",
            "Processing row 117...\n",
            "\n",
            "Processing row 118...\n",
            "\n",
            "Processing row 119...\n",
            "\n",
            "Processing row 120...\n",
            "\n",
            "Processing row 121...\n",
            "\n",
            "Processing row 122...\n",
            "\n",
            "Processing row 123...\n",
            "\n",
            "Processing row 124...\n",
            "\n",
            "Processing row 125...\n",
            "\n",
            "Processing row 126...\n",
            "\n",
            "Processing row 127...\n",
            "\n",
            "Processing row 128...\n",
            "\n",
            "Processing row 129...\n",
            "\n",
            "Processing row 130...\n",
            "\n",
            "Processing row 131...\n",
            "\n",
            "Processing row 132...\n",
            "\n",
            "Processing row 133...\n",
            "\n",
            "Processing row 134...\n",
            "\n",
            "Processing row 135...\n",
            "\n",
            "Processing row 136...\n",
            "\n",
            "Processing row 137...\n",
            "\n",
            "Processing row 138...\n",
            "\n",
            "Processing row 139...\n",
            "\n",
            "Processing row 140...\n",
            "\n",
            "Processing row 141...\n",
            "\n",
            "Processing row 142...\n",
            "\n",
            "Processing row 143...\n",
            "\n",
            "Processing row 144...\n",
            "\n",
            "Processing row 145...\n",
            "\n",
            "Processing row 146...\n",
            "\n",
            "Processing row 147...\n",
            "\n",
            "Processing row 148...\n",
            "\n",
            "Processing row 149...\n",
            "\n",
            "Processing row 150...\n",
            "Saving results to /content/SampleRedundancyCases_Updated.xlsx\n",
            "Processing complete!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModel, pipeline\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Download NLTK tokenizer models (only needed once)\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Load model and tokenizer for embeddings\n",
        "embedding_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "print(f\"Loading embedding model: {embedding_model_name}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
        "embedding_model = AutoModel.from_pretrained(embedding_model_name)\n",
        "\n",
        "# Load NLI model\n",
        "nli_model_name = \"roberta-large-mnli\"\n",
        "print(f\"Loading NLI model: {nli_model_name}\")\n",
        "nli_pipeline = pipeline(\"text-classification\", model=nli_model_name, return_all_scores=True)\n",
        "\n",
        "# Function to split text into sentences using NLTK\n",
        "def split_into_sentences(passage):\n",
        "    sentences = sent_tokenize(passage)\n",
        "    return sentences\n",
        "\n",
        "# Function to calculate embeddings\n",
        "def calculate_embeddings(sentences):\n",
        "    inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        embeddings = embedding_model(**inputs).last_hidden_state.mean(dim=1)\n",
        "    return embeddings.numpy()\n",
        "\n",
        "# Function to calculate cosine similarity for a passage\n",
        "def calculate_cosine_similarity(passage):\n",
        "    sentences = split_into_sentences(passage)\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    if num_sentences < 2:\n",
        "        return 0.0  # Return 0 if fewer than 2 sentences\n",
        "\n",
        "    embeddings = calculate_embeddings(sentences)\n",
        "    similarity_matrix = cosine_similarity(embeddings)\n",
        "    pairwise_similarities = similarity_matrix[np.triu_indices(num_sentences, k=1)]\n",
        "    avg_similarity = np.mean(pairwise_similarities)\n",
        "    return round(avg_similarity, 4)\n",
        "\n",
        "# Function to calculate lexical overlap for a passage\n",
        "def calculate_lexical_overlap(passage):\n",
        "    sentences = split_into_sentences(passage)\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    if num_sentences < 2:\n",
        "        return 0.0  # Return 0 if fewer than 2 sentences\n",
        "\n",
        "    # Tokenize words using word_tokenize for better accuracy\n",
        "    combined_tokens = word_tokenize(\" \".join(sentences).lower())\n",
        "    token_counts = Counter(combined_tokens)\n",
        "    repeated_tokens = sum(count - 1 for count in token_counts.values() if count > 1)\n",
        "    total_tokens = len(combined_tokens)\n",
        "    overlap_ratio = repeated_tokens / total_tokens if total_tokens > 0 else 0\n",
        "    return round(overlap_ratio, 4)\n",
        "\n",
        "# Function to calculate NLI score matrices for a passage\n",
        "def calculate_nli_matrices(passage):\n",
        "    sentences = split_into_sentences(passage)\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    if num_sentences < 2:\n",
        "        return 0.0, 0.0, 0.0  # Return zeros if fewer than 2 sentences\n",
        "\n",
        "    entailment_matrix = np.zeros((num_sentences, num_sentences))\n",
        "    neutral_matrix = np.zeros((num_sentences, num_sentences))\n",
        "    contradiction_matrix = np.zeros((num_sentences, num_sentences))\n",
        "\n",
        "    for i in range(num_sentences):\n",
        "        for j in range(num_sentences):\n",
        "            if i != j:  # Avoid self-comparison\n",
        "                premise = sentences[i]\n",
        "                hypothesis = sentences[j]\n",
        "                result = nli_pipeline(f\"{premise} [SEP] {hypothesis}\")\n",
        "\n",
        "                for score in result[0]:\n",
        "                    if score[\"label\"] == \"ENTAILMENT\":\n",
        "                        entailment_matrix[i, j] = score[\"score\"]\n",
        "                    elif score[\"label\"] == \"NEUTRAL\":\n",
        "                        neutral_matrix[i, j] = score[\"score\"]\n",
        "                    elif score[\"label\"] == \"CONTRADICTION\":\n",
        "                        contradiction_matrix[i, j] = score[\"score\"]\n",
        "\n",
        "    entailment_score = np.mean(entailment_matrix[np.triu_indices(num_sentences, k=1)])\n",
        "    neutral_score = np.mean(neutral_matrix[np.triu_indices(num_sentences, k=1)])\n",
        "    contradiction_score = np.mean(contradiction_matrix[np.triu_indices(num_sentences, k=1)])\n",
        "\n",
        "    return round(entailment_score, 4), round(neutral_score, 4), round(contradiction_score, 4)\n",
        "\n",
        "# Function to calculate bidirectional NLI score matrices for a passage\n",
        "def calculate_nli_matrices_bidirectional(passage):\n",
        "    sentences = split_into_sentences(passage)\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    if num_sentences < 2:\n",
        "        return 0.0, 0.0, 0.0  # Return zeros if fewer than 2 sentences\n",
        "\n",
        "    # Initialize matrices\n",
        "    entailment_matrix = np.zeros((num_sentences, num_sentences))\n",
        "    neutral_matrix = np.zeros((num_sentences, num_sentences))\n",
        "    contradiction_matrix = np.zeros((num_sentences, num_sentences))\n",
        "\n",
        "    for i in range(num_sentences):\n",
        "        for j in range(num_sentences):\n",
        "            if i != j:  # Avoid self-comparison\n",
        "                # First direction: Premise = i, Hypothesis = j\n",
        "                result_1 = nli_pipeline(f\"{sentences[i]} [SEP] {sentences[j]}\")\n",
        "                # Second direction: Premise = j, Hypothesis = i\n",
        "                result_2 = nli_pipeline(f\"{sentences[j]} [SEP] {sentences[i]}\")\n",
        "\n",
        "                # Aggregate scores for both directions\n",
        "                for score in result_1[0]:\n",
        "                    if score[\"label\"] == \"ENTAILMENT\":\n",
        "                        entailment_matrix[i, j] += score[\"score\"]\n",
        "                    elif score[\"label\"] == \"NEUTRAL\":\n",
        "                        neutral_matrix[i, j] += score[\"score\"]\n",
        "                    elif score[\"label\"] == \"CONTRADICTION\":\n",
        "                        contradiction_matrix[i, j] += score[\"score\"]\n",
        "\n",
        "                for score in result_2[0]:\n",
        "                    if score[\"label\"] == \"ENTAILMENT\":\n",
        "                        entailment_matrix[i, j] += score[\"score\"]\n",
        "                    elif score[\"label\"] == \"NEUTRAL\":\n",
        "                        neutral_matrix[i, j] += score[\"score\"]\n",
        "                    elif score[\"label\"] == \"CONTRADICTION\":\n",
        "                        contradiction_matrix[i, j] += score[\"score\"]\n",
        "\n",
        "                # Normalize the scores (average for the two directions)\n",
        "                entailment_matrix[i, j] /= 2\n",
        "                neutral_matrix[i, j] /= 2\n",
        "                contradiction_matrix[i, j] /= 2\n",
        "\n",
        "    # Extract upper triangular values (excluding the diagonal)\n",
        "    entailment_score = np.mean(entailment_matrix[np.triu_indices(num_sentences, k=1)])\n",
        "    neutral_score = np.mean(neutral_matrix[np.triu_indices(num_sentences, k=1)])\n",
        "    contradiction_score = np.mean(contradiction_matrix[np.triu_indices(num_sentences, k=1)])\n",
        "\n",
        "    return round(entailment_score, 4), round(neutral_score, 4), round(contradiction_score, 4)\n",
        "\n",
        "# Load the Excel file\n",
        "input_file = \"/content/SampleRedundancyCases.xlsx\"  # Change to your file path\n",
        "output_file = \"/content/SampleRedundancyCases_Updated.xlsx\"  # Output file path\n",
        "\n",
        "print(f\"Reading input file: {input_file}\")\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Ensure result columns exist\n",
        "if \"Cosine Similarity\" not in df.columns:\n",
        "    df[\"Cosine Similarity\"] = \"\"\n",
        "if \"Lexical Overlap\" not in df.columns:\n",
        "    df[\"Lexical Overlap\"] = \"\"\n",
        "if \"Entailment Score\" not in df.columns:\n",
        "    df[\"Entailment Score\"] = \"\"\n",
        "if \"Neutral Score\" not in df.columns:\n",
        "    df[\"Neutral Score\"] = \"\"\n",
        "if \"Contradiction Score\" not in df.columns:\n",
        "    df[\"Contradiction Score\"] = \"\"\n",
        "if \"Dominant Score\" not in df.columns:\n",
        "    df[\"Dominant Score\"] = \"\"\n",
        "if \"Entailment Score (Bi)\" not in df.columns:\n",
        "    df[\"Entailment Score (Bi)\"] = \"\"\n",
        "if \"Neutral Score (Bi)\" not in df.columns:\n",
        "    df[\"Neutral Score (Bi)\"] = \"\"\n",
        "if \"Contradiction Score (Bi)\" not in df.columns:\n",
        "    df[\"Contradiction Score (Bi)\"] = \"\"\n",
        "if \"Dominant Score (Bi)\" not in df.columns:\n",
        "    df[\"Dominant Score (Bi)\"] = \"\"\n",
        "\n",
        "# Process each row\n",
        "print(\"Starting processing with both NLI methods...\")\n",
        "for index, row in df.iterrows():\n",
        "    sentence1 = row.get(\"Sentence 1\", \"\")\n",
        "    sentence2 = row.get(\"Sentence 2\", \"\")\n",
        "\n",
        "    if pd.notna(sentence1) and pd.notna(sentence2):\n",
        "        passage = f\"{sentence1} {sentence2}\"\n",
        "        print(f\"\\nProcessing row {index + 1}...\")\n",
        "\n",
        "        # Calculate similarities and scores\n",
        "        cosine_sim = calculate_cosine_similarity(passage)\n",
        "        lexical_overlap = calculate_lexical_overlap(passage)\n",
        "        entailment_score, neutral_score, contradiction_score = calculate_nli_matrices(passage)\n",
        "        entailment_score_bi, neutral_score_bi, contradiction_score_bi = calculate_nli_matrices_bidirectional(passage)\n",
        "\n",
        "        # Determine the dominant score for original NLI\n",
        "        scores = {\n",
        "            \"Entailment\": entailment_score,\n",
        "            \"Neutral\": neutral_score,\n",
        "            \"Contradiction\": contradiction_score,\n",
        "        }\n",
        "        dominant_score = max(scores, key=scores.get)\n",
        "\n",
        "        # Determine the dominant score for bidirectional NLI\n",
        "        scores_bi = {\n",
        "            \"Entailment (Bi)\": entailment_score_bi,\n",
        "            \"Neutral (Bi)\": neutral_score_bi,\n",
        "            \"Contradiction (Bi)\": contradiction_score_bi,\n",
        "        }\n",
        "        dominant_score_bi = max(scores_bi, key=scores_bi.get)\n",
        "\n",
        "        # Save results\n",
        "        df.at[index, \"Cosine Similarity\"] = cosine_sim\n",
        "        df.at[index, \"Lexical Overlap\"] = lexical_overlap\n",
        "        df.at[index, \"Entailment Score\"] = entailment_score\n",
        "        df.at[index, \"Neutral Score\"] = neutral_score\n",
        "        df.at[index, \"Contradiction Score\"] = contradiction_score\n",
        "        df.at[index, \"Dominant Score\"] = dominant_score\n",
        "        df.at[index, \"Entailment Score (Bi)\"] = entailment_score_bi\n",
        "        df.at[index, \"Neutral Score (Bi)\"] = neutral_score_bi\n",
        "        df.at[index, \"Contradiction Score (Bi)\"] = contradiction_score_bi\n",
        "        df.at[index, \"Dominant Score (Bi)\"] = dominant_score_bi\n",
        "    else:\n",
        "        print(f\"Skipping row {index + 1}: Missing sentence(s)\")\n",
        "\n",
        "# Save the results back to Excel\n",
        "print(f\"Saving results to {output_file}\")\n",
        "df.to_excel(output_file, index=False)\n",
        "print(\"Processing complete!\")\n"
      ]
    }
  ]
}